{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TrainingRunnner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- setup dataloader\n",
    "- start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.706Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.715Z"
    }
   },
   "outputs": [],
   "source": [
    "#export \n",
    "from exp.nb_CustomDataLoader import Detection_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.720Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip3 install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.726Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(\"/Users/ckh/Documents/Poladrone/nb/ref/pytorchYOLOv4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.731Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torch import optim\n",
    "# from tensorboardX import SummaryWriter\n",
    "# import logging\n",
    "# import os, sys\n",
    "# from tqdm import tqdm\n",
    "# # from dataset import Yolo_dataset\n",
    "# # from cfg import Cfg__init\n",
    "# from ref.pytorchYOLOv4.models import Yolov4\n",
    "\n",
    "# import argparse\n",
    "# from easydict import EasyDict as edict\n",
    "# from torch.nn import functional as F\n",
    "\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.737Z"
    }
   },
   "outputs": [],
   "source": [
    "from ref.pytorchYOLOv4.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.746Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    images = []\n",
    "    bboxes = []\n",
    "    for i, (img, box) in enumerate(batch):\n",
    "        images.append([img])\n",
    "        \n",
    "        box = np.concatenate((box, [[i]]*len(box)), 1)\n",
    "        bboxes.append(box)\n",
    "        \n",
    "    images = np.concatenate(images, axis=0)\n",
    "    images = images.transpose(0, 3, 1, 2)\n",
    "    images = torch.from_numpy(images).div(255.0)\n",
    "            \n",
    "    bboxes = np.concatenate(bboxes)\n",
    "\n",
    "    bboxes = torch.from_numpy(bboxes)\n",
    "    return images, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.754Z"
    },
    "code_folding": [
     22,
     39
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "def custom_train(train_dataset, val_dataset, model, device, config, epochs=5, batch_size=1, save_cp=True, log_step=20, img_scale=0.5):\n",
    "#     train_dataset = Yolo_dataset(config.train_label, config)\n",
    "#     val_dataset = Yolo_dataset(config.val_label, config)\n",
    "\n",
    "    n_train = len(train_dataset)\n",
    "    n_val = len(val_dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch // config.subdivisions, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True, drop_last=True, collate_fn=custom_collate)\n",
    "\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=config.batch // config.subdivisions, shuffle=True, num_workers=1,\n",
    "#                             pin_memory=True, drop_last=True)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=config.TRAIN_TENSORBOARD_DIR,\n",
    "                           filename_suffix=f'OPT_{config.TRAIN_OPTIMIZER}_LR_{config.learning_rate}_BS_{config.batch}_Sub_{config.subdivisions}_Size_{config.width}',\n",
    "                           comment=f'OPT_{config.TRAIN_OPTIMIZER}_LR_{config.learning_rate}_BS_{config.batch}_Sub_{config.subdivisions}_Size_{config.width}')\n",
    "    # writer.add_images('legend',\n",
    "    #                   torch.from_numpy(train_dataset.label2colorlegend2(cfg.DATA_CLASSES).transpose([2, 0, 1])).to(\n",
    "    #                       device).unsqueeze(0))\n",
    "    max_itr = config.TRAIN_EPOCHS * n_train\n",
    "    # global_step = cfg.TRAIN_MINEPOCH * n_train\n",
    "    global_step = 0\n",
    "    logging.info(f'''Starting training:\n",
    "        Epochs:          {epochs}\n",
    "        Batch size:      {config.batch}\n",
    "        Subdivisions:    {config.subdivisions}\n",
    "        Learning rate:   {config.learning_rate}\n",
    "        Training size:   {n_train}\n",
    "        Validation size: {n_val}\n",
    "        Checkpoints:     {save_cp}\n",
    "        Device:          {device.type}\n",
    "        Images size:     {config.width}\n",
    "        Optimizer:       {config.TRAIN_OPTIMIZER}\n",
    "        Dataset classes: {config.classes}\n",
    "        Train label path:{config.train_label}\n",
    "        Pretrained:\n",
    "    ''')\n",
    "\n",
    "    # learning rate setup\n",
    "    def burnin_schedule(i):\n",
    "        if i < config.burn_in:\n",
    "            factor = pow(i / config.burn_in, 4)\n",
    "        elif i < config.steps[0]:\n",
    "            factor = 1.0\n",
    "        elif i < config.steps[1]:\n",
    "            factor = 0.1\n",
    "        else:\n",
    "            factor = 0.01\n",
    "        return factor\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.learning_rate / config.batch, betas=(0.9, 0.999), eps=1e-08)\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, burnin_schedule)\n",
    "\n",
    "    criterion = Yolo_loss(n_classes=config.classes, device=device, batch=config.batch // config.subdivisions)\n",
    "    # scheduler = ReduceLROnPlateau(optimizer, mode='max', verbose=True, patience=6, min_lr=1e-7)\n",
    "    # scheduler = CosineAnnealingWarmRestarts(optimizer, 0.001, 1e-6, 20)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        #model.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_step = 0\n",
    "\n",
    "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img', ncols=50) as pbar:\n",
    "            for i, batch in enumerate(train_loader):\n",
    "                global_step += 1\n",
    "                epoch_step += 1\n",
    "                images = batch[0]\n",
    "                bboxes = batch[1]\n",
    "\n",
    "                images = images.to(device=device, dtype=torch.float32)\n",
    "                bboxes = bboxes.to(device=device)\n",
    "\n",
    "                bboxes_pred = model(images)\n",
    "                loss, loss_xy, loss_wh, loss_obj, loss_cls, loss_l2 = criterion(bboxes_pred, bboxes)\n",
    "                # loss = loss / config.subdivisions\n",
    "                loss.backward()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                if global_step  % config.subdivisions == 0:\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    model.zero_grad()\n",
    "\n",
    "                if global_step % (log_step * config.subdivisions) == 0:\n",
    "                    writer.add_scalar('train/Loss', loss.item(), global_step)\n",
    "                    writer.add_scalar('train/loss_xy', loss_xy.item(), global_step)\n",
    "                    writer.add_scalar('train/loss_wh', loss_wh.item(), global_step)\n",
    "                    writer.add_scalar('train/loss_obj', loss_obj.item(), global_step)\n",
    "                    writer.add_scalar('train/loss_cls', loss_cls.item(), global_step)\n",
    "                    writer.add_scalar('train/loss_l2', loss_l2.item(), global_step)\n",
    "                    writer.add_scalar('lr', scheduler.get_lr()[0] * config.batch, global_step)\n",
    "                    pbar.set_postfix(**{'loss (batch)': loss.item(), 'loss_xy': loss_xy.item(),\n",
    "                                        'loss_wh': loss_wh.item(),\n",
    "                                        'loss_obj': loss_obj.item(),\n",
    "                                        'loss_cls': loss_cls.item(),\n",
    "                                        'loss_l2': loss_l2.item(),\n",
    "                                        'lr': scheduler.get_lr()[0] * config.batch\n",
    "                                        })\n",
    "                    logging.debug('Train step_{}: loss : {},loss xy : {},loss wh : {},'\n",
    "                                  'loss obj : {}ï¼Œloss cls : {},loss l2 : {},lr : {}'\n",
    "                                  .format(global_step, loss.item(), loss_xy.item(),\n",
    "                                          loss_wh.item(), loss_obj.item(),\n",
    "                                          loss_cls.item(), loss_l2.item(),\n",
    "                                          scheduler.get_lr()[0] * config.batch))\n",
    "\n",
    "                pbar.update(images.shape[0])\n",
    "\n",
    "            if save_cp:\n",
    "                try:\n",
    "                    os.mkdir(config.checkpoints)\n",
    "                    logging.info('Created checkpoint directory')\n",
    "                except OSError:\n",
    "                    pass\n",
    "                torch.save(model.state_dict(), os.path.join(config.checkpoints, f'Yolov4_epoch{epoch + 1}.pth'))\n",
    "                logging.info(f'Checkpoint {epoch + 1} saved !')\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.760Z"
    }
   },
   "outputs": [],
   "source": [
    "from config.config import Cfg\n",
    "data_list = \"/Users/ckh/OneDrive - Default Directory/Hui_Wan/train_npt.txt\"\n",
    "dataset = Detection_dataset(data_list, Cfg)\n",
    "dataset.names\n",
    "\n",
    "\n",
    "## without augmentation\n",
    "val_dataset = Detection_dataset(data_list, Cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.766Z"
    }
   },
   "outputs": [],
   "source": [
    "logging = init_logger(log_dir='log', stdout=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.771Z"
    }
   },
   "outputs": [],
   "source": [
    "cfg = get_args(**Cfg)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = cfg.gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logging.info(f'Using device {device}')\n",
    "\n",
    "model = Yolov4(cfg.pretrained,n_classes=cfg.classes)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "model.to(device=device)\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.800Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "torch.manual_seed(20)\n",
    "np.random.seed(20)\n",
    "random.seed(20)\n",
    "\n",
    "\n",
    "try:\n",
    "    custom_train(\n",
    "        train_dataset=dataset,\n",
    "        val_dataset=val_dataset,\n",
    "        model=model,\n",
    "        config=cfg,\n",
    "        epochs=cfg.TRAIN_EPOCHS,\n",
    "        device=device, )\n",
    "except KeyboardInterrupt:\n",
    "    torch.save(model.state_dict(), 'INTERRUPTED.pth')\n",
    "    logging.info('Saved interrupt')\n",
    "    try:\n",
    "        sys.exit(0)\n",
    "    except SystemExit:\n",
    "        os._exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.804Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "torch.manual_seed(20)\n",
    "np.random.seed(20)\n",
    "random.seed(20)\n",
    "\n",
    "[len(dataset[i][1]) for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.810Z"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"NOTEBOOK = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.815Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python notebook2script.py \"$NOTEBOOK\".ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-07-01T15:25:16.820Z"
    }
   },
   "outputs": [],
   "source": [
    "%%javascripddt\n",
    "var kernel = IPython.notebook.kernel;\n",
    "var thename = window.document.getElementById(\"notebook_name\").innerHTML;\n",
    "var command = \"NOTEBOOK = \" + \"'\"+thename+\"'\";\n",
    "kernel.execute(command);\n",
    "\n",
    "!python notebook2script.py \"$NOTEBOOK\".ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
